{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries required for natural language processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.no</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Web-domain</th>\n",
       "      <th>Flare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4991</td>\n",
       "      <td>4992</td>\n",
       "      <td>‘Why Fight Over Ayodhya?’ Kids Explore Growing...</td>\n",
       "      <td>ashuhitman1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4992</td>\n",
       "      <td>4993</td>\n",
       "      <td>What is more important - the Babri mosque, the...</td>\n",
       "      <td>Rauf_Will_Speak</td>\n",
       "      <td>4.0</td>\n",
       "      <td>self.india</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4993</td>\n",
       "      <td>4994</td>\n",
       "      <td>Can you fill a 2 minute survey about GST for m...</td>\n",
       "      <td>king9karan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>self.india</td>\n",
       "      <td>Policy/Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4994</td>\n",
       "      <td>4995</td>\n",
       "      <td>This is a very pressing issue that we must dis...</td>\n",
       "      <td>Throwaway_Mattress</td>\n",
       "      <td>2.0</td>\n",
       "      <td>self.india</td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4995</td>\n",
       "      <td>4996</td>\n",
       "      <td>Will Diwali fireworks damage house structures ...</td>\n",
       "      <td>riverfellon</td>\n",
       "      <td>3.0</td>\n",
       "      <td>self.india</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4996</td>\n",
       "      <td>4997</td>\n",
       "      <td>Diwali has been dull this time: Priyanka Gandh...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>timesofindia.indiatimes.com</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4997</td>\n",
       "      <td>4998</td>\n",
       "      <td>Harbhajan or Bumrah? This Girl's Unique Bowlin...</td>\n",
       "      <td>Savi321</td>\n",
       "      <td>2.0</td>\n",
       "      <td>in.news.yahoo.com</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>4999</td>\n",
       "      <td>Huge gap in private, government hospital Ayush...</td>\n",
       "      <td>zistu</td>\n",
       "      <td>3.0</td>\n",
       "      <td>timesofindia.indiatimes.com</td>\n",
       "      <td>Policy/Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4999</td>\n",
       "      <td>5000</td>\n",
       "      <td>It’s a great time to be an influencer on Insta...</td>\n",
       "      <td>scribbbblr</td>\n",
       "      <td>3.0</td>\n",
       "      <td>amp.scroll.in</td>\n",
       "      <td>Business/Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5001</td>\n",
       "      <td>Defence Minister Rajnath Singh, in Haryana's K...</td>\n",
       "      <td>Gavthi_Batman</td>\n",
       "      <td>2.0</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      S.no                                              Title  \\\n",
       "4991  4992  ‘Why Fight Over Ayodhya?’ Kids Explore Growing...   \n",
       "4992  4993  What is more important - the Babri mosque, the...   \n",
       "4993  4994  Can you fill a 2 minute survey about GST for m...   \n",
       "4994  4995  This is a very pressing issue that we must dis...   \n",
       "4995  4996  Will Diwali fireworks damage house structures ...   \n",
       "4996  4997  Diwali has been dull this time: Priyanka Gandh...   \n",
       "4997  4998  Harbhajan or Bumrah? This Girl's Unique Bowlin...   \n",
       "4998  4999  Huge gap in private, government hospital Ayush...   \n",
       "4999  5000  It’s a great time to be an influencer on Insta...   \n",
       "5000  5001  Defence Minister Rajnath Singh, in Haryana's K...   \n",
       "\n",
       "                  Author  Likes                   Web-domain             Flare  \n",
       "4991         ashuhitman1    0.0                  youtube.com          Politics  \n",
       "4992     Rauf_Will_Speak    4.0                   self.india          AskIndia  \n",
       "4993          king9karan    1.0                   self.india    Policy/Economy  \n",
       "4994  Throwaway_Mattress    2.0                   self.india     Non-Political  \n",
       "4995         riverfellon    3.0                   self.india          AskIndia  \n",
       "4996           [deleted]    2.0  timesofindia.indiatimes.com          Politics  \n",
       "4997             Savi321    2.0            in.news.yahoo.com            Sports  \n",
       "4998               zistu    3.0  timesofindia.indiatimes.com    Policy/Economy  \n",
       "4999          scribbbblr    3.0                amp.scroll.in  Business/Finance  \n",
       "5000       Gavthi_Batman    2.0                  twitter.com          Politics  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading data from csv file\n",
    "df = pd.read_csv(r'C:\\Users\\admin\\Anaconda3\\Scripts\\Reddit flare prediction\\data.csv')\n",
    "df.dropna(inplace = True)\n",
    "df.reset_index(inplace  = True)\n",
    "df['S.no'] = df['S.no'].astype(\"int32\")\n",
    "del df['index']\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining some topics which should have same topic\n",
    "polt = [\"Politics\",\"CAA-NRC-NPR\",\"Politics [Megathread]\",\"CAA-NRC\",\"Politics -- Source in comments\"]\n",
    "no_polt = [\"Non-Political\",\"40 Martyrs\",\"Goal Achieved!!!\",\"On Internet Shutdowns\"]\n",
    "policy = [\"Policy/Economy\",\"Demonetization\",\"Policy & Economy\",\"Policy/Economy -2017 Article\",\"[Year: 2001] Policy/Economy\",\"Policy/Economy [Megathread]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting common Flare for common topics\n",
    "df.loc[df[\"Flare\"].isin(polt),\"Flare\"] = \"Politics\"\n",
    "df.loc[df[\"Flare\"].isin(no_polt),\"Flare\"] = \"Non-Political\"\n",
    "df.loc[df[\"Flare\"].isin(policy),\"Flare\"] = \"Policy/Economy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking top 6 topics having most number of data points\n",
    "df = df.loc[df[\"Flare\"].isin(df.Flare.unique()[:6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop_words:\n",
    "            w = w.lower()\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w,pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word)\n",
    "            \n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_flare(str):\n",
    "    if str==\"Coronavirus\":\n",
    "        return 1\n",
    "    elif str == \"Politics\":\n",
    "        return 2\n",
    "    elif str==\"Non-Political\":\n",
    "        return 3\n",
    "    elif str==\"AskIndia\":\n",
    "        return 4\n",
    "    elif str == \"Policy/Economy\":\n",
    "        return 5\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Title\"].values\n",
    "Y = df[\"Flare\"].apply(change_flare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iit ia study make depressed able sleep please help',\n",
       " 'please help ’ go insane lockdown end',\n",
       " 'rajasthan cease use rapid test kit result invalid health minister raghu sharma say kit give mere five per cent correct valid result report forward indian council medical research icmr regard',\n",
       " 'pregnant muslim woman refuse treatment india force give birth stillborn beaten',\n",
       " 'isa global immigration agency scam',\n",
       " 'first coronavirus patient receive plasma therapy recovers take ventilator delhi hospital',\n",
       " \"pakistani student sadhguru call 'taliban name forbes list\",\n",
       " 'communal riot india past 5 year mukhtar abbas naqvi',\n",
       " 'hello find good piano teacher online',\n",
       " \"`` thank doctor kerala safer '' italian tourist covid 19 recovery\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "X = [ clean_review(word_tokenize(\" \".join(i.split('-')))) for i in X]\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 4601)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X),len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "remove_index = []\n",
    "for i in range(0,len(X)):\n",
    "    if len(X[i].split())<2:\n",
    "        remove_index.append(i)\n",
    "\n",
    "for i in range(len(remove_index),0,-1):\n",
    "    X.pop(i)\n",
    "    Y.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting data points in train,validation and test set\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,random_state = 0,test_size = 0.3)\n",
    "x_val,x_test,y_val,y_test = train_test_split(x_test,y_test,test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CountVectorizer to preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 4000,ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = cv.fit_transform(x_train)\n",
    "x_val_vec = cv.transform(x_val)\n",
    "x_test_vec = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBClassifier = MultinomialNB(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBClassifier.fit(x_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8601508485229415\n",
      "0.5586510263929618\n",
      "0.5695461200585652\n"
     ]
    }
   ],
   "source": [
    "print(NBClassifier.score(x_train_vec,y_train))\n",
    "print(NBClassifier.score(x_val_vec,y_val))\n",
    "print(NBClassifier.score(x_test_vec,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 5000,ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = cv.fit_transform(x_train)\n",
    "x_val_vec = cv.transform(x_val)\n",
    "x_test_vec = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBClassifier = MultinomialNB(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8796354494028913"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBClassifier.fit(x_train_vec,y_train)\n",
    "NBClassifier.score(x_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5645161290322581"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBClassifier.score(x_val_vec,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tf-Idf Vectorizer to preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 7000 , ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = tfidf.fit_transform(x_train)\n",
    "x_val_vec = tfidf.transform(x_val)\n",
    "x_test_vec = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBClassifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7661847894406034"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBClassifier.fit(x_train_vec,y_train)\n",
    "NBClassifier.score(x_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5586510263929618"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = NBClassifier.predict(x_val_vec)\n",
    "NBClassifier.score(x_val_vec,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.63      0.76      0.69       221\n",
      "           2       0.51      0.77      0.61       189\n",
      "           3       0.54      0.40      0.46       161\n",
      "           4       0.29      0.03      0.05        71\n",
      "           5       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.56       682\n",
      "   macro avg       0.33      0.33      0.30       682\n",
      "weighted avg       0.50      0.56      0.51       682\n",
      "\n",
      "[[  0   6   2   5   0   0]\n",
      " [  0 168  41  10   2   0]\n",
      " [  0  29 146  14   0   0]\n",
      " [  0  32  61  65   3   0]\n",
      " [  0  18  27  24   2   0]\n",
      " [  0  15  10   2   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val,y_pred))\n",
    "print(confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "                       n_jobs=None, oob_score=False, random_state=100,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators = 50, random_state = 100)\n",
    "classifier.fit(x_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5747800586510264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.15      0.25        13\n",
      "           1       0.71      0.70      0.71       221\n",
      "           2       0.61      0.63      0.62       189\n",
      "           3       0.43      0.60      0.51       161\n",
      "           4       0.41      0.21      0.28        71\n",
      "           5       0.60      0.11      0.19        27\n",
      "\n",
      "    accuracy                           0.57       682\n",
      "   macro avg       0.57      0.40      0.43       682\n",
      "weighted avg       0.58      0.57      0.56       682\n",
      "\n",
      "[[  2   5   0   6   0   0]\n",
      " [  0 155  30  32   4   0]\n",
      " [  0  21 120  46   1   1]\n",
      " [  1  14  32  97  17   0]\n",
      " [  0  11   9  35  15   1]\n",
      " [  0  11   6   7   0   3]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(x_val_vec)\n",
    "print(classifier.score(x_val_vec,y_val))\n",
    "print(classification_report(y_val,y_pred))\n",
    "print(confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952859836580766\n",
      "0.6002928257686676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.09      0.14        22\n",
      "           1       0.74      0.74      0.74       221\n",
      "           2       0.62      0.67      0.64       183\n",
      "           3       0.47      0.62      0.53       154\n",
      "           4       0.49      0.29      0.36        76\n",
      "           5       0.57      0.15      0.24        27\n",
      "\n",
      "    accuracy                           0.60       683\n",
      "   macro avg       0.54      0.43      0.44       683\n",
      "weighted avg       0.60      0.60      0.59       683\n",
      "\n",
      "[[  2   2   4  10   4   0]\n",
      " [  2 163  26  24   6   0]\n",
      " [  1  10 123  44   3   2]\n",
      " [  0  22  25  96  10   1]\n",
      " [  1  19  12  22  22   0]\n",
      " [  0   5   9   9   0   4]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(x_test_vec)\n",
    "print(classifier.score(x_train_vec,y_train))\n",
    "print(classifier.score(x_test_vec,y_test))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel = 'linear', C = 1.5,gamma = 0.01)\n",
    "svc.fit(x_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947517284726587\n",
      "0.6041055718475073\n"
     ]
    }
   ],
   "source": [
    "print(svc.score(x_train_vec,y_train))\n",
    "print(svc.score(x_val_vec,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6222547584187409"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svc.predict(x_test_vec)\n",
    "svc.score(x_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.09      0.15        22\n",
      "           1       0.73      0.76      0.74       221\n",
      "           2       0.64      0.73      0.68       183\n",
      "           3       0.52      0.58      0.55       154\n",
      "           4       0.48      0.37      0.42        76\n",
      "           5       0.50      0.15      0.23        27\n",
      "\n",
      "    accuracy                           0.62       683\n",
      "   macro avg       0.55      0.45      0.46       683\n",
      "weighted avg       0.61      0.62      0.61       683\n",
      "\n",
      "[[  2   3   3   9   4   1]\n",
      " [  1 167  25  23   5   0]\n",
      " [  1  15 134  27   4   2]\n",
      " [  0  19  30  90  14   1]\n",
      " [  0  17  12  19  28   0]\n",
      " [  1   7   6   6   3   4]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(x_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8485229415461973\n",
      "0.5733137829912024\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(x_train_vec,y_train))\n",
    "print(clf.score(x_val_vec,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666178623718887"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing some parameters in count vectorizer to increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 7000,analyzer = 'word',stop_words = stop_words,ngram_range = (1,4),max_df = 0.8)\n",
    "x_train_vec = cv.fit_transform(x_train)\n",
    "x_val_vec = cv.transform(x_val)\n",
    "x_test_vec = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5542521994134897\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel = 'linear', C = 1.5,gamma = 0.01)\n",
    "svc.fit(x_train_vec,y_train)\n",
    "print(svc.score(x_val_vec,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5762463343108505"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(x_train_vec,y_train)\n",
    "clf.score(x_val_vec,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5762463343108505"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 50, random_state = 100)\n",
    "classifier.fit(x_train_vec,y_train)\n",
    "classifier.score(x_val_vec,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing some parameters in Tf-Idf vectorizer to increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 7000 , ngram_range=(1,4),analyzer = 'word',stop_words = stop_words,max_df = 0.8,lowercase = True)\n",
    "x_train_vec = tfidf.fit_transform(x_train)\n",
    "x_val_vec = tfidf.transform(x_val)\n",
    "x_test_vec = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5967741935483871\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel = 'linear', C = 1.6,gamma = 0.01)\n",
    "svc.fit(x_train_vec,y_train)\n",
    "print(svc.score(x_val_vec,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5718475073313783"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(x_train_vec,y_train)\n",
    "clf.score(x_val_vec,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5850439882697948"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 50, random_state = 100)\n",
    "classifier.fit(x_train_vec,y_train)\n",
    "classifier.score(x_val_vec,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266471449487555"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(x_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5592972181551976"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5885797950219619"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(x_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3182, 6)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_val = np.array(y_val).reshape(len(y_val), 1)\n",
    "y_test = np.array(y_test).reshape(len(y_test), 1)\n",
    "\n",
    "y_train_encoded = onehot_encoder.fit_transform(y_train)\n",
    "y_val_encoded = onehot_encoder.fit_transform(y_val)\n",
    "y_test_encoded = onehot_encoder.fit_transform(y_test)\n",
    "y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3182 samples, validate on 682 samples\n",
      "Epoch 1/15\n",
      "3182/3182 [==============================] - 2s 694us/step - loss: 0.5933 - accuracy: 0.8161 - val_loss: 0.4573 - val_accuracy: 0.8333\n",
      "Epoch 2/15\n",
      "3182/3182 [==============================] - 2s 502us/step - loss: 0.4032 - accuracy: 0.8338 - val_loss: 0.3717 - val_accuracy: 0.8350\n",
      "Epoch 3/15\n",
      "3182/3182 [==============================] - 2s 504us/step - loss: 0.3402 - accuracy: 0.8508 - val_loss: 0.3377 - val_accuracy: 0.8568\n",
      "Epoch 4/15\n",
      "3182/3182 [==============================] - 2s 498us/step - loss: 0.2880 - accuracy: 0.8797 - val_loss: 0.3160 - val_accuracy: 0.8729\n",
      "Epoch 5/15\n",
      "3182/3182 [==============================] - 2s 505us/step - loss: 0.2423 - accuracy: 0.9060 - val_loss: 0.3048 - val_accuracy: 0.8744\n",
      "Epoch 6/15\n",
      "3182/3182 [==============================] - 2s 499us/step - loss: 0.2050 - accuracy: 0.9272 - val_loss: 0.3023 - val_accuracy: 0.8724\n",
      "Epoch 7/15\n",
      "3182/3182 [==============================] - 2s 514us/step - loss: 0.1744 - accuracy: 0.9409 - val_loss: 0.3035 - val_accuracy: 0.8700\n",
      "Epoch 8/15\n",
      "3182/3182 [==============================] - 2s 498us/step - loss: 0.1493 - accuracy: 0.9522 - val_loss: 0.3097 - val_accuracy: 0.8661\n",
      "Epoch 9/15\n",
      "3182/3182 [==============================] - 2s 521us/step - loss: 0.1277 - accuracy: 0.9609 - val_loss: 0.3168 - val_accuracy: 0.8671\n",
      "Epoch 10/15\n",
      "3182/3182 [==============================] - 2s 531us/step - loss: 0.1109 - accuracy: 0.9661 - val_loss: 0.3261 - val_accuracy: 0.8644\n",
      "Epoch 11/15\n",
      "3182/3182 [==============================] - 2s 497us/step - loss: 0.0942 - accuracy: 0.9747 - val_loss: 0.3363 - val_accuracy: 0.8649\n",
      "Epoch 12/15\n",
      "3182/3182 [==============================] - 2s 516us/step - loss: 0.0820 - accuracy: 0.9782 - val_loss: 0.3471 - val_accuracy: 0.8636\n",
      "Epoch 13/15\n",
      "3182/3182 [==============================] - 2s 506us/step - loss: 0.0718 - accuracy: 0.9814 - val_loss: 0.3582 - val_accuracy: 0.8624\n",
      "Epoch 14/15\n",
      "3182/3182 [==============================] - 2s 532us/step - loss: 0.0625 - accuracy: 0.9857 - val_loss: 0.3706 - val_accuracy: 0.8617\n",
      "Epoch 15/15\n",
      "3182/3182 [==============================] - 2s 489us/step - loss: 0.0565 - accuracy: 0.9876 - val_loss: 0.3816 - val_accuracy: 0.8607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c71c8efcc8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64,activation='relu', input_dim=7000))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train_vec, y_train_encoded,epochs=15, validation_data=(x_val_vec, y_val_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 0s 101us/step\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(x_test_vec,y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3578313812504401\n",
      "0.8757930994033813\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above neural network gives accuracy of 87.5%. Thus it is working decently well as compared to primitive ML algorithms which were giving accuracy in range of 57-63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(y_pred):\n",
    "    res = []\n",
    "    for i in y_pred:\n",
    "        res.append(np.argmax(i))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.09      0.15        22\n",
      "           1       0.71      0.69      0.70       221\n",
      "           2       0.59      0.70      0.64       183\n",
      "           3       0.48      0.56      0.52       154\n",
      "           4       0.52      0.42      0.47        76\n",
      "           5       0.44      0.15      0.22        27\n",
      "\n",
      "    accuracy                           0.59       683\n",
      "   macro avg       0.54      0.44      0.45       683\n",
      "weighted avg       0.59      0.59      0.58       683\n",
      "\n",
      "[[  2   1   4   9   4   2]\n",
      " [  1 152  34  28   5   1]\n",
      " [  0  18 129  30   5   1]\n",
      " [  0  23  29  86  15   1]\n",
      " [  0  13  14  17  32   0]\n",
      " [  1   6   7   9   0   4]]\n"
     ]
    }
   ],
   "source": [
    "#Classification report of neural network\n",
    "y_pred = model.predict(x_test_vec)\n",
    "y_pred = fun(y_pred)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above neural network works really well on classes 1-4(Coronavirus,Politics,Non-Political,AskIndia), maybe due to presence of large number of data-points belonging to these classes. While classes 0 & 5 are performing not that well as there are less data points belonging to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tf-Idf.pkl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Serializing model and vectorizer for web application\n",
    "joblib.dump(model,'NN_model.pkl')\n",
    "joblib.dump(tfidf,'Tf-Idf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
